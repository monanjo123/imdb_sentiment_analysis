{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monanjo123/imdb_sentiment_analysis/blob/master/IMDB_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n_Dj2QXR8CD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c002dc4-7267-4f3b-ce0f-1168acd15ca4"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, GlobalAveragePooling1D, LSTM"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SexHQrhT6yU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "a041303d-c61e-408b-f317-772175ce4122"
      },
      "source": [
        "# Load IMDB dataset\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=10000,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "print(x_train[0], y_train[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTxUaeRWgY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7f4629b1-f606-4560-9844-2a82bc013768"
      },
      "source": [
        "# Convert Integers back to words\n",
        "\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "  \n",
        "decode_review(x_train[1])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rx-Z3v3W9Zi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f23a4f80-7982-421c-a463-c8c3bd442561"
      },
      "source": [
        "# Sequence Padding\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=100, value=0.)\n",
        "x_test = pad_sequences(x_test, maxlen=100, value=0.)\n",
        "\n",
        "print(len(x_train), len(x_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eETgS3SRXyoB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "outputId": "fcb269f3-6772-46ae-e5ed-78ed15c2822c"
      },
      "source": [
        "# build model\n",
        "vocab_size = 10000\n",
        "batch_size = 16\n",
        "epochs = 40\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, batch_size))\n",
        "model.add(LSTM(500, return_sequences=True))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Dense(16))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "             optimizer='rmsprop',\n",
        "             metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0901 15:25:47.182031 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0901 15:25:47.219594 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0901 15:25:47.225748 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0901 15:25:47.842751 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0901 15:25:47.850210 139675243480960 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0901 15:25:47.871402 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0901 15:25:47.886034 139675243480960 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0901 15:25:47.890596 139675243480960 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, None, 16)          160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 500)         1034000   \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_1 ( (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16)                8016      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,202,033\n",
            "Trainable params: 1,202,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MIOEB-VaSXO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f1785f22-c99e-4d7b-a0fa-89567a0c5edc"
      },
      "source": [
        "# Train Model\n",
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]\n",
        "\n",
        "history = model.fit(x_train, \n",
        "                    y_train,\n",
        "                    batch_size=512,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    shuffle=True\n",
        "                   )\n",
        "# model.save('/drive/My Drive/Machine Learning/Models/cat_dog_classifier_data_augmented.h5')\n",
        "\n",
        "# Evaluate the performance of our trained model\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/40\n",
            "25000/25000 [==============================] - 14s 575us/step - loss: 0.6205 - acc: 0.7376 - val_loss: 0.5760 - val_acc: 0.7245\n",
            "Epoch 2/40\n",
            "25000/25000 [==============================] - 14s 575us/step - loss: 0.5618 - acc: 0.7876 - val_loss: 0.4940 - val_acc: 0.7519\n",
            "Epoch 3/40\n",
            "25000/25000 [==============================] - 14s 579us/step - loss: 0.5131 - acc: 0.8183 - val_loss: 0.5320 - val_acc: 0.8076\n",
            "Epoch 4/40\n",
            "25000/25000 [==============================] - 15s 582us/step - loss: 0.4429 - acc: 0.8406 - val_loss: 0.4080 - val_acc: 0.8246\n",
            "Epoch 5/40\n",
            "25000/25000 [==============================] - 15s 585us/step - loss: 0.4118 - acc: 0.8558 - val_loss: 0.4019 - val_acc: 0.8258\n",
            "Epoch 6/40\n",
            "25000/25000 [==============================] - 15s 589us/step - loss: 0.3596 - acc: 0.8725 - val_loss: 0.4101 - val_acc: 0.8104\n",
            "Epoch 7/40\n",
            "25000/25000 [==============================] - 15s 592us/step - loss: 0.3244 - acc: 0.8850 - val_loss: 0.3873 - val_acc: 0.8323\n",
            "Epoch 8/40\n",
            "25000/25000 [==============================] - 15s 594us/step - loss: 0.3010 - acc: 0.8968 - val_loss: 0.3887 - val_acc: 0.8316\n",
            "Epoch 9/40\n",
            "25000/25000 [==============================] - 15s 597us/step - loss: 0.2914 - acc: 0.9016 - val_loss: 0.3903 - val_acc: 0.8350\n",
            "Epoch 10/40\n",
            "25000/25000 [==============================] - 15s 599us/step - loss: 0.2692 - acc: 0.9074 - val_loss: 0.3745 - val_acc: 0.8336\n",
            "Epoch 11/40\n",
            "25000/25000 [==============================] - 15s 601us/step - loss: 0.2545 - acc: 0.9140 - val_loss: 0.4194 - val_acc: 0.8278\n",
            "Epoch 12/40\n",
            "25000/25000 [==============================] - 15s 604us/step - loss: 0.2449 - acc: 0.9160 - val_loss: 0.4341 - val_acc: 0.8258\n",
            "Epoch 13/40\n",
            "25000/25000 [==============================] - 15s 608us/step - loss: 0.2370 - acc: 0.9225 - val_loss: 0.4528 - val_acc: 0.8307\n",
            "Epoch 14/40\n",
            "25000/25000 [==============================] - 15s 616us/step - loss: 0.2213 - acc: 0.9288 - val_loss: 0.4988 - val_acc: 0.8266\n",
            "Epoch 15/40\n",
            "25000/25000 [==============================] - 15s 614us/step - loss: 0.2212 - acc: 0.9302 - val_loss: 0.4157 - val_acc: 0.8299\n",
            "Epoch 16/40\n",
            "25000/25000 [==============================] - 15s 611us/step - loss: 0.2048 - acc: 0.9359 - val_loss: 0.4140 - val_acc: 0.8152\n",
            "Epoch 17/40\n",
            "25000/25000 [==============================] - 15s 611us/step - loss: 0.2015 - acc: 0.9389 - val_loss: 0.4245 - val_acc: 0.8254\n",
            "Epoch 18/40\n",
            "25000/25000 [==============================] - 15s 611us/step - loss: 0.1934 - acc: 0.9396 - val_loss: 0.4718 - val_acc: 0.8197\n",
            "Epoch 19/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1871 - acc: 0.9426 - val_loss: 0.5052 - val_acc: 0.8240\n",
            "Epoch 20/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1817 - acc: 0.9447 - val_loss: 0.5951 - val_acc: 0.8178\n",
            "Epoch 21/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1771 - acc: 0.9468 - val_loss: 0.5039 - val_acc: 0.8146\n",
            "Epoch 22/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1729 - acc: 0.9478 - val_loss: 0.5421 - val_acc: 0.8168\n",
            "Epoch 23/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1646 - acc: 0.9500 - val_loss: 0.4680 - val_acc: 0.8207\n",
            "Epoch 24/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1616 - acc: 0.9519 - val_loss: 0.6145 - val_acc: 0.8076\n",
            "Epoch 25/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1567 - acc: 0.9529 - val_loss: 0.5797 - val_acc: 0.8167\n",
            "Epoch 26/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1524 - acc: 0.9553 - val_loss: 0.5909 - val_acc: 0.8155\n",
            "Epoch 27/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1501 - acc: 0.9562 - val_loss: 0.6422 - val_acc: 0.8140\n",
            "Epoch 28/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1411 - acc: 0.9603 - val_loss: 0.6116 - val_acc: 0.8136\n",
            "Epoch 29/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1422 - acc: 0.9574 - val_loss: 0.7076 - val_acc: 0.8117\n",
            "Epoch 30/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1384 - acc: 0.9595 - val_loss: 0.6794 - val_acc: 0.7881\n",
            "Epoch 31/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1309 - acc: 0.9625 - val_loss: 0.8183 - val_acc: 0.8014\n",
            "Epoch 32/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1321 - acc: 0.9607 - val_loss: 0.7523 - val_acc: 0.7912\n",
            "Epoch 33/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1254 - acc: 0.9647 - val_loss: 0.7033 - val_acc: 0.8072\n",
            "Epoch 34/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1228 - acc: 0.9633 - val_loss: 0.8231 - val_acc: 0.7968\n",
            "Epoch 35/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1253 - acc: 0.9636 - val_loss: 0.8300 - val_acc: 0.8000\n",
            "Epoch 36/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1176 - acc: 0.9668 - val_loss: 0.7921 - val_acc: 0.8027\n",
            "Epoch 37/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1146 - acc: 0.9666 - val_loss: 0.9150 - val_acc: 0.7938\n",
            "Epoch 38/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1124 - acc: 0.9683 - val_loss: 0.7980 - val_acc: 0.8026\n",
            "Epoch 39/40\n",
            "25000/25000 [==============================] - 15s 612us/step - loss: 0.1104 - acc: 0.9686 - val_loss: 0.8383 - val_acc: 0.7908\n",
            "Epoch 40/40\n",
            "25000/25000 [==============================] - 15s 613us/step - loss: 0.1107 - acc: 0.9688 - val_loss: 0.8884 - val_acc: 0.7981\n",
            "25000/25000 [==============================] - 24s 979us/step\n",
            "Test loss: 0.8883858419179916\n",
            "Test accuracy: 0.79812\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}